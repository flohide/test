{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3TfOaicqCI"
      },
      "source": [
        "<figure>\n",
        "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
        "</figure>\n",
        "\n",
        "# Machine Learning\n",
        "### Sommersemester 2025\n",
        "Prof. Dr. Stefan Goetze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTKRTmcfcqCL"
      },
      "source": [
        "# Rückblick Logistic Regression\n",
        "\n",
        "In diesem Aufgabenblatt wollen wir nochmal auf die Logistische Regression zurückblicken und aus dem Linearen Modell ein eifaches neuronales Netz herleiten.\n",
        "Dazu schauen wir uns nochmal im Detail an, wie die Modell- und Kostenfunktionen der logistischen Regression aufgebaut sind und implementieren das Gradientenverfahren zur Minimierung der Kostenfunktion per Hand (d.h. mit Python bzw. NumPy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FXcj0MjWcqCM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E3VTWHgicqCO"
      },
      "outputs": [],
      "source": [
        "# Test Case\n",
        "m_c, n_c = 20, 4\n",
        "np.random.seed(0)\n",
        "X_c = np.random.randn(m_c, n_c)\n",
        "theta0_c, theta1_c = 0, np.random.randn(n_c, 1)\n",
        "y_c = np.random.randn(m_c, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JowkwquAcqCO"
      },
      "source": [
        "Bei der Logistischen Regression haben wir das Gradientenverfahren benutzt, um die Parameter unseres Modells, einer linearen Funktion $$Z = f_{\\Theta}(x)=\\Theta_0+\\Theta_1x$$ transformiert durch die Aktivierungsfunktion $$\\hat y = h_{\\Theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$ schrittweise zu verbessern.\n",
        "Die Verbesserung, bzw. die Qualität des Modells, haben wir anhand der Kostenfunktion $$J_{\\Theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$ berechnet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nb3t3bgcqCP"
      },
      "source": [
        "## Modellfunktion\n",
        "**Aufgabe: Schreibe eine Funktion $f$, die folgende Parameter erhält:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält.\n",
        "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
        "\n",
        "**$f$ soll folgende lineare Funktion implementieren:** $$Z = f_{\\Theta}(X)=\\Theta_0+X\\Theta_1$$\n",
        "\n",
        "*Hinweis: Denken Sie daran, dass Matrix-Multiplikationen in NumPy mit dem @-Operator implementiert werden können.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a46e169efb324584",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7r_NkiMUcqCQ"
      },
      "outputs": [],
      "source": [
        "def f(X, theta0, theta1):\n",
        "    \"\"\"evaluates linear function.\\n\"\n",
        "    Arguments:\\n\",\n",
        "        X: value\\n\",\n",
        "        theta0: Bias\\n\",\n",
        "        theta1: Function slope\\n\",\n",
        "    \"\"\"\n",
        "    Z=theta0+X @ theta1\n",
        "    return Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-798b7b367b206c2f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "93JZJTC7cqCQ"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "Z = f(X_c, theta0_c, theta1_c)\n",
        "#----------\n",
        "# f\n",
        "\n",
        "assert Z.shape == (m_c, 1), 'Use correctly sequenced matrix multiplication'\n",
        "assert np.isclose(Z[0], -4.68171, atol=0.001), 'Expected -4.68171 but got %.5f' %Z[0]\n",
        "\n",
        "del Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNfSsH2QcqCR"
      },
      "source": [
        "**Aufgabe: Implementieren Sie die Perzepton-Fuinkion $h$. Die Funktion $h$ soll die gleichen Parameter wie $f$ erhalten und die Funktion $f$ intern aufrufen. Das Ergebnis von $f$ soll durch die Sigmoid-Aktivierungsfunktion transformiert werden.:** $$\\hat y = h_{\\Theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$\n",
        "\n",
        "*Hinweis: Die Exponentialfunktion angewendet auf ein Argument $z$ können Sie in NumPy mit dem Methodenaufruf `np.exp(z)` berechnen.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-aed34bc92bba34fb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_9F_ExsCcqCR"
      },
      "outputs": [],
      "source": [
        "def h(X, theta0, theta1):\n",
        "    \"\"\"returns the sigmoid of the linear function.\n",
        "    Arguments:\\n\",\n",
        "        X: Data\\n\",\n",
        "        theta0: Bias\\n\",\n",
        "        theta1: Function slope\\n\",\n",
        "    \"\"\"\n",
        "    Z=f(X, theta0, theta1)\n",
        "    y_hat= 1/(1+np.exp(-Z))\n",
        "    return y_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3680cf0b32123e22",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wzffXVrCcqCS"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "y_hat = h(X_c, theta0_c, theta1_c)\n",
        "#----------\n",
        "# h\n",
        "\n",
        "assert y_hat.shape == (m_c, 1)\n",
        "assert np.isclose(y_hat[0], 0.00918, atol=0.001), 'Expected 0.00918 but got %.5f' %y_hat[0]\n",
        "\n",
        "del y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffzyZbF9cqCS"
      },
      "source": [
        "## Kostenfunktion\n",
        "\n",
        "**Aufgabe: Berechnen Sie nun die Kostenfunktion. Schreiben Sie eine Funktion $J$, die folgende Parameter erhält:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "\n",
        "**$J$ berechnet die folgende Kostenfunktion:** $$J_{\\Theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$\n",
        "\n",
        "\n",
        "*Hinweis: Sie können die Summe in zwei Teilsummen aufteilen und so die Berechnung vektorisieren:*\n",
        "\n",
        " $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})] = -\\frac{1}{m}  \\left( \\sum\\limits_{i = 1}^{m} y^{(i)}\\log(\\hat y^{(i)}) + \\sum\\limits_{i = 1}^{m}(1-y^{(i)})\\log(1- \\hat y^{(i)})\\right)$$\n",
        "\n",
        "*und $\\sum\\limits_{i = 1}^{m} y^{(i)}\\log(\\hat y^{(i)})$ ist einfach das Skalarprodukt aus $y$ und $\\log(\\hat y)$.*\n",
        "\n",
        "*Den natürlichen Logarithmus von `x` berechnen Sie in NumPy mit `np.log(x)`, das Skalarprodukt mit `np.dot(a,b)` oder, falls `a` und `b` (2-dimensionale) Spaltenvektoren sind, einfach mit der Matrizenmultiplikation `a.T@b` (`a` wird transponiert, um einen Zeilenvektor zu erhalten).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2441e8b76f6c8b3b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Vv2bU-XqcqCS"
      },
      "outputs": [],
      "source": [
        "def J(X,theta0, theta1, y):\n",
        "  \"\"\"computes the Cross-entropy cost function\\n\",\n",
        "  Arguments:\n",
        "  X: Data\n",
        "  theta0: Bias\n",
        "  theta1: Function slope\n",
        "  y: True labels\n",
        "  \"\"\"\n",
        "  y_hat=h(X, theta0, theta1)\n",
        "  m=np.shape(y_c)[0]\n",
        "  J=-1/(m)*(y.T@(np.log(y_hat))+(1-y).T@(np.log(1-y_hat)))\n",
        "  return J.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-6d386b0ed985b5a1",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dgXBduHGcqCS"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "cost = J(X_c,theta0_c,theta1_c,y_c)\n",
        "#----------\n",
        "# J\n",
        "\n",
        "assert cost.shape == (), 'Use correctly sequenced matrix multiplication'\n",
        "assert np.isclose(cost, 1.49867, atol=0.001), 'Expected 0.66739 but got %.5f' %cost\n",
        "\n",
        "del cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhE77uROcqCS"
      },
      "source": [
        "## Gradientenverfahren\n",
        "\n",
        "**Schreiben Sie für das Gradientenverfahren eine Funktion `grads`, die folgende Parameter erhält**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "\n",
        "**und den Fradienten $\\partial\\theta$ für die Parameter $\\theta$ berechnet**\n",
        "\n",
        "Dabei ist $\\partial\\theta_0$ ein Skalar der dem Gradienten des Bias Parameters entspricht: $$\\partial\\theta_0 = \\frac{1}{m} \\sum_{i=1}^m (\\hat y^{(i)}-y^{(i)})$$\n",
        "\n",
        "$\\partial\\theta_1$ ist ein Vektor der Dimension `(n, 1)` mit den Gradienten der anderen Parameter: $$ \\partial \\theta_1 = \\frac{1}{m}X^T(\\hat y-y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-467565ebb154795c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "FAkkRxKtcqCT"
      },
      "outputs": [],
      "source": [
        "def grads(X,theta0, theta1,y):\n",
        "    \"\"\"Calculates grads of the cost function with respect to the parameters.\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta0: Bias\n",
        "        theta1: Function slope\n",
        "        y: True labels\n",
        "    \"\"\"\n",
        "    y_hat=h(X, theta0, theta1)\n",
        "    m=np.shape(y_c)[0]\n",
        "    dtheta0=(1/m*np.sum(y_hat-y))\n",
        "    dtheta1=1/m*X.T@(y_hat-y)\n",
        "    return dtheta0, dtheta1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d5aaa7214b2dfbcd",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "fXjWGvW_cqCT"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "dt0, dt1 = grads(X_c,theta0_c,theta1_c,y_c)\n",
        "#----------\n",
        "# grads\n",
        "\n",
        "assert dt0.shape == (), 'theta0 should be a scaler'\n",
        "assert dt1.shape == theta1_c.shape\n",
        "assert np.isclose(dt1[0], -0.22445, atol=0.001), 'Expected -0.22445 but got %.5f' %dt1[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9Nk0U_cqCT"
      },
      "source": [
        "**Aufgabe: Schreiben Sie nun eine Funktion, die die Modellparameter aufgrund der berechneten Gradienten aktualisiert.Die Funktion `update`erhält die Parameter $\\theta$, die Gradienten $\\partial \\theta$ sowie die Lernrate $\\alpha$ und berechnet:**\n",
        "\n",
        "$$\\theta = \\theta - \\alpha \\cdot \\partial \\theta$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9c9c665b5ef00354",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "I50ckp6YcqCT"
      },
      "outputs": [],
      "source": [
        "def update(theta0, theta1,dtheta0, dtheta1, alpha):\n",
        "  \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
        "\n",
        "  theta0=theta0-alpha*dtheta0\n",
        "  theta1=theta1-alpha*dtheta1\n",
        "  return theta0, theta1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-628e4db830095cf8",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VMgbMdY6cqCT"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "t0, t1 = update(theta0_c, theta1_c,dt0, dt1, 0.1)\n",
        "#----------\n",
        "# update\n",
        "\n",
        "assert t0.shape == (), 'theta0 should be a scaler'\n",
        "assert t1.shape == theta1_c.shape\n",
        "assert np.isclose(t1[0], -1.14270, atol=0.001), 'Expected -0.141491 but got %.5f' %t1[0]\n",
        "del t0, t1, dt0, dt1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUL--wh2cqCT"
      },
      "source": [
        "Nun können wir das iterative Gradientenverfahren programmieren.\n",
        "\n",
        "**Aufgabe: Schreiben Sie eine Funktion `gradient_descent`, die folgende Parameter erhät:**\n",
        "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
        "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
        "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
        "4. Die Lernrate $\\alpha$.\n",
        "5. Die Anzahl der Iterationen.\n",
        "**Die Funktion soll die Trainierten Modellparameter $\\theta$ zurückgeben.**\n",
        "\n",
        "*Hinweis*: Berechnen Sie Kosten mit der Funktion `J` und hängen Sie diese Kosten nach jedem BErechnungsschritt and die Liste `cost` an --> Berechnen Sie die Gradienten mit der Funktion `grads` --> Verwenden Sie diese Gradienten um die Parameter mit der Funktion `optim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c950a15eb73cda45",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wlht8nExcqCT"
      },
      "outputs": [],
      "source": [
        "def gradient_decent(X, theta0, theta1, y, alpha=0.1, iterations=400):\n",
        "    \"\"\"performs gradient decent optimization.\n",
        "    Arguments:\n",
        "        X: Data\n",
        "        theta0: Bias\n",
        "        theta1: Function slope\n",
        "        y: True labels\n",
        "        alpha(default=0.1): Learning rate\n",
        "        iterations(default=400): number of updating iterations\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "      J_cost=J(X, theta0, theta1, y)\n",
        "      costs.append(J_cost)\n",
        "      gradss=grads(X, theta0, theta1, y)\n",
        "      theta0, theta1=update(theta0, theta1, gradss[0], gradss[1], alpha)\n",
        "    return theta0, theta1, costs\n",
        "\n",
        "\n",
        "    return theta0, theta1, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-9e057a811aa97078",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "g_1kqb5fcqCT"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "t0, t1, costs = gradient_decent(X_c, theta0_c, theta1_c, y_c)\n",
        "#----------\n",
        "# gradient_decent\n",
        "\n",
        "assert len(costs) == 400, 'Make sure to calculate and append the cost in every iteration.'\n",
        "assert np.isclose(t1[3], 2.93724, atol=0.001), 'Expected 1.05186 but got %.5f' %t1[3]\n",
        "\n",
        "del t0, t1, costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogwUySgqcqCT"
      },
      "source": [
        "## Beispiel\n",
        "\n",
        "Wir habe nun alle Funktionen um unser Perzeptron für einen *echten* Datensatz zu einzusetzen.\n",
        "Wir verwenden hier den Brustkrebs-Datensatz aus Sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sCsjQk5acqCT"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,test_size=0.3)\n",
        "\n",
        "# preprocessing\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "y_train = np.expand_dims(y_train, 1)\n",
        "y_test = np.expand_dims(y_test, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn5W20f3cqCU",
        "outputId": "96ac5bf4-5d01-4f34-9c1f-0c56cbe2a789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f37980db79cd>:11: RuntimeWarning: divide by zero encountered in log\n",
            "  J=-1/(m)*(y.T@(np.log(y_hat))+(1-y).T@(np.log(1-y_hat)))\n",
            "<ipython-input-13-f37980db79cd>:11: RuntimeWarning: invalid value encountered in matmul\n",
            "  J=-1/(m)*(y.T@(np.log(y_hat))+(1-y).T@(np.log(1-y_hat)))\n"
          ]
        }
      ],
      "source": [
        "# initializing parameters\n",
        "theta0 = 0.\n",
        "theta1 = np.zeros((len(X_train[0]), 1))\n",
        "\n",
        "#training the model\n",
        "theta0, theta1, costs = gradient_decent(X_train, theta0, theta1, y_train.reshape(-1, 1), alpha = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "A6aW1Xv-cqCU",
        "outputId": "207885bc-c90a-4e93-9a0f-999c31449cc3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CINDXuXxcqCU",
        "outputId": "47d3e647-046a-4d7e-8d5b-11f53e633b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die classifcation accuracy ist:  98.83040935672514\n"
          ]
        }
      ],
      "source": [
        "# measuring performance\n",
        "y_pred = (h(X_test,theta0, theta1) >= 0.5)*1\n",
        "acc = 100-np.sum(np.abs(y_pred-y_test))*100/len(y_test)\n",
        "\n",
        "\n",
        "print(\"Die classifcation accuracy ist: \",acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TP8Ai-tcqCU"
      },
      "source": [
        "# Neurale Netze\n",
        "\n",
        "Bisher haben wir ein Perzeptron betrachtet, also eine Art Neuronales Netz mit nur einer Schicht. Die gleichen Methoden können wir aber auch für mehrschichtige Neuronale Netze verwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOvrgYUDcqCU"
      },
      "source": [
        "## Forward Pass\n",
        "\n",
        "![](https://github.com/fh-swf-hgi/ml/raw/main/u6/nn-matrices.png)\n",
        "\n",
        "**Die Lineare Funktion $f$ unserer Netzwerkschicht erhält folgende Parameter:**\n",
        "1. Die Matrix $A \\in \\mathbb{R}^{n^{[i]}\\times{}m}$, mit den Eingabedaten der Netzwerkschicht $i$. Die Zeilen von $A$ enthalten die einzelnen Datenpunkte. Die Spalten von $A$ entsprechend den Merkmalen (und damit den Parametern) eines voll-vermaschten Neurons der aktuellen Schicht.\n",
        "2. Einem Vektor $b \\in \\mathbb{R}^{n^{[i]}}$ mit den Bias Parametern der Netzwerkschicht $i$.\n",
        "3. Die Matrix $W \\in \\mathbb{R}^{n^{[i-1]}\\times{}n^{[i]}}$ mit den Gewichten der Netzwerkschicht $i$.\n",
        "\n",
        "**und liefert folgendes Ergebnis $Z$ zurück:** $$Z = f_{\\Theta}(A)= b + AW$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ex9xZAucqCU"
      },
      "outputs": [],
      "source": [
        "def f(A, b, W):\n",
        "    \"\"\"evaluates linear function.\n",
        "    Arguments:\n",
        "        A: Layer input data\n",
        "        b: Biases vector\n",
        "        W: Weights matrix\n",
        "    \"\"\"\n",
        "    Z = b + A@W\n",
        "    cache = (A, b, W) # wir merken uns die Werte A, b, und W in jeder Schicht des netzes\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gMZm0MycqCU"
      },
      "source": [
        "**Als Aktivierungsfunktion verwenden wir die (nichtlineare) Sigmoid-Funktion.\n",
        "Die Funktion erhält die gleichen Parameter wie  `f` und ruft `f` auch intern auf:**\n",
        "$$A^{[i+1]}= h_{\\Theta}(A^{[i]}) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uDJnam3cqCU"
      },
      "outputs": [],
      "source": [
        "def h(A, b, W):\n",
        "    \"\"\"computes the sigmoid of the linear function.\n",
        "    Arguments:\n",
        "        A: Layer input data\n",
        "        b: Biases vector\n",
        "        W: Weights matrix\n",
        "    \"\"\"\n",
        "\n",
        "    Z, Zcache = f(A, b, W)\n",
        "    A = 1.0 / (1.0+np.exp(-Z))\n",
        "    cache = (Zcache, Z) # Zu der Eingabe der Schicht merken wir uns im cache den wert der linearen Funktion f\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj1rgXUMcqCU"
      },
      "source": [
        "Wie die Abbildung oben zeigt, ist die Größe des Netzes durch die Anzahl der Neuronen auf jeder Schicht festgelegt.\n",
        "Kennt man die Anzahl der Neuronen, kann man die Parameter-Matrizen $W$ und die Vektoren für die Bias Parameter $b$ initialisieren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy1fzp5TcqCU"
      },
      "outputs": [],
      "source": [
        "def init(dims):\n",
        "    \"\"\"returns initial weights and biases.\"\"\"\n",
        "    #dims = [X,L1,L2,...,y]\n",
        "    b = []\n",
        "    W = []\n",
        "\n",
        "    for l in range(1, len(dims)):\n",
        "        b.append(np.zeros((1, dims[l])))\n",
        "        W.append(np.random.randn(dims[l-1], dims[l])*0.01) # to break symmetry, it's good to have non-zero weights\n",
        "\n",
        "    return b, W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5BkEZeUcqCU"
      },
      "source": [
        "Da wir nun die Modellfunktionen für die einezelnen Schichten erstellt haben (und unser Netzwerk auf allen Schichten die gleiche Aktivierungsfunktion nutzt), können wir nun recht einfach eine Funktion für den gesamten **Forward Pass** definieren.\n",
        "Die Funktion `forward` erhält als Parameter die Eingabedaten `X`, sowie **Listen** der Bias-Parameter und der Modellparameter für die einzelnen Schichten. Beachten Sie, dass jedes Element der Listen einen Vektor, bzw. eine Matrix mit den Parametern einer Schicht enthält.\n",
        "Die Länge dieser Listen entspricht also der Tiefe des Neuronalen Netzes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnpZ1HV8cqCV"
      },
      "outputs": [],
      "source": [
        "def forward(X, b, W):\n",
        "    \"\"\"Performs a forward propagation through all layers.\n",
        "    returns the output layer and cache of intermidiates.\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "\n",
        "    for l in range(len(b)):\n",
        "        A_prev = A\n",
        "        A, cache = h(A_prev, b[l], W[l])\n",
        "        # Hänge den cache jeder Schicht an die Liste Caches an\n",
        "        caches.append(cache)\n",
        "\n",
        "    return A, caches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-roy1cDBcqCV"
      },
      "source": [
        "Nachdem wir am Ende des Forward Passes unsere Schätzung berechnet haben, müssen wir für diese Schätzung die Kosten berechnen.\n",
        "Die Funktion `J` erhält folgende Parameter:\n",
        "1. Die vom Modell vorhergesagten Label $\\hat{y}$ in der Größe des Datensatzes (1, m)\n",
        "3. Die tatsächlichen Label unseres Datensatzes $y$, ebenfalls in der Größe (1, m)\n",
        "\n",
        "**Als Kostenfunktion verwenden wir wie schon bei der Logistischen Regression die Kreuzentropie:**\n",
        "$$J(W,b)-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1- \\hat{y}^{(i)})]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orTcAIe_cqCV"
      },
      "outputs": [],
      "source": [
        "def J(y_pred, y):\n",
        "    \"\"\"evaluates Cross-entropy cost function.\"\"\"\n",
        "    J = -(y.T@np.log(y_pred) + ((1.0-y.T)@np.log(1.0-y_pred)))/len(y)\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv3KZos_cqCa"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Der Backpropagation (zu dt. *Fehlerrückführung*) Algorithmus ist ein Spezialfall des Gradientenverfahrens und das am meisten eingesetzte Verfahren zum Trainieren neuronaler Netze.\n",
        "\n",
        "Um Backpropagation zu implementieren definieren wir eine Funktion mit folgenden Parametern:\n",
        "1. Den Gradienten der Kosten nach der Aktivierungsfunktion ($\\partial A$)\n",
        "2. Die zwischengespeicherten Werten $f$ und $h$ der Aktivierungs- und Übertragungsfunktionen aus dem Forward Pass.\n",
        "\n",
        "Zurückgeliefert werden die Gradienten $\\partial b, \\partial W, und \\partial A_{prev}$:\n",
        "\n",
        "$\\partial b$ ist ein Vektor mit den Gradienten der Bias Parameter: $$\\partial b = \\frac{1}{m} \\sum_{i=1}^m (\\partial z^{(i)})$$\n",
        "\n",
        "\n",
        "$\\partial W$ ist eine Matrix mit den Gradienten der Gewichte: $$ \\partial W = \\frac{1}{m}A_{prev}^T\\partial Z$$\n",
        "\n",
        "$\\partial A_{prev}$ ist der Grantient der Kostenfunktion bezüglich der Aktivierungsfunktion $A$ der nächstvorderen Schicht:  $$\\partial A_{prev} = \\frac{1}{m}\\partial ZW^T$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9LviAq8cqCa"
      },
      "outputs": [],
      "source": [
        "def back(dA, cache):\n",
        "    \"\"\"calculates grads w.r.t. linear function.\"\"\"\n",
        "\n",
        "    Zcache, Z = cache\n",
        "\n",
        "    sigma = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * sigma * (1-sigma)\n",
        "\n",
        "    A_prev, b, W = Zcache\n",
        "\n",
        "    m = len(A_prev[0])\n",
        "    db = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "    dW = (A_prev.T@dZ)/m\n",
        "    dA_prev = (dZ@W.T)/m\n",
        "\n",
        "    return dA_prev, db, dW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6npWiYQcqCa"
      },
      "source": [
        "**Wir können nun die Funktion `back` auf allen Schichten des Modells, beginnend mit der Ausgabeschicht und endend auf der Eingabeschicht aufrufen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U1q016_cqCa"
      },
      "outputs": [],
      "source": [
        "def grads(y_hat, y, caches):\n",
        "    \"\"\"performs backprobagation through all layers to calculate grads of all parameters.\"\"\"\n",
        "\n",
        "    db = []\n",
        "    dW = []\n",
        "\n",
        "    dA = -(y/y_hat - (1-y)/(1-y_hat)) # Gradient der Kostenfunktion auf der Ausgabeschicht\n",
        "\n",
        "    for l in reversed(range(len(caches))):\n",
        "        # Backward step\n",
        "        dA, db_, dW_ = back(dA, caches[l])\n",
        "\n",
        "        db.insert(0, db_)\n",
        "        dW.insert(0, dW_)\n",
        "\n",
        "    return db, dW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2AdmYgscqCa"
      },
      "source": [
        "## Optimization\n",
        "\n",
        "Für das Gradientenverfahren benötigen wir noch eine Funktion, die uns die Parameter jeder Schicht, basierend auf den berechneten Gradienten anpasst.\n",
        "Diese Funktion `optim` erhält als Parameter:\n",
        "1. Die Parameter des Neuronalen Netzes ($b, W$),\n",
        "2. die Gradienten der einzelnen Schichten ($\\partial b, \\partial W$)\n",
        "3. sowie die Lernrate $\\alpha$.\n",
        "\n",
        "Die Funktion liefert die aktualisierten Parameter ($b, W$) zurück."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcJVBReTcqCa"
      },
      "outputs": [],
      "source": [
        "def optim(parameters, grads, alpha):\n",
        "    \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
        "\n",
        "    b, W = parameters\n",
        "    db, dW = grads\n",
        "\n",
        "    for l in range(len(b)):\n",
        "        b[l] = b[l] - alpha*db[l]\n",
        "        W[l] = W[l] - alpha*dW[l]\n",
        "\n",
        "    return b, W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92GTSuXHcqCa"
      },
      "source": [
        "## Ein Modell Trainieren\n",
        "\n",
        "Wir können nun unser selbst programmiertes MLP Netzwerk mit einem echten Datensatz trainieren. Wir verwenden hier den bekannten MNIST Datensatz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N96FYhscqCa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from tensorflow.keras import datasets , utils\n",
        "except:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install --upgrade pip\n",
        "    !{sys.executable} -m pip install tensorflow\n",
        "    #!{sys.executable} -m pip install tensorflow-macos tensorflow-metal\n",
        "    from tensorflow.keras import datasets , utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdMoV78WcqCb"
      },
      "outputs": [],
      "source": [
        "#Importing MNIST from tensorflow dataset liabrary\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3q_WLucqCb"
      },
      "source": [
        "Wenn wir den Datensatz geladen haben, können wir uns einige Beispiele der Daten anzeigen lassen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVqDzGPVcqCb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 10, figsize=(18,3))\n",
        "\n",
        "for i in range(10):\n",
        "    ax[i].set_title('Image label: %d' %i)\n",
        "    ax[i].axis('off')\n",
        "    ax[i].imshow(X_train[y_train == i][0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9GybQwpcqCb"
      },
      "source": [
        "Für unser MLP Netzwerk machen wir aus den $28\\times{}28$ Pixel großen Bildern der Ziffern eindimensionale Vektoren der Länge $748$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQDR5dcmcqCb"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "print(\"Vorher:\", X_train.shape)\n",
        "X_train = X_train.reshape(-1, 28*28)/255\n",
        "X_test = X_test.reshape(-1, 28*28)/255\n",
        "y_train = utils.to_categorical(y_train)\n",
        "y_test = utils.to_categorical(y_test)\n",
        "print(\"Nachher:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GiEkiy2cqCb"
      },
      "source": [
        "Nun stellen wir aus allen zuvor definierten Funktionen unser Modell auf:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGhZp-SpcqCb"
      },
      "outputs": [],
      "source": [
        "def model(X, y, layers_dims, alpha = 0.01, epoch = 20, batch_size = 400):\n",
        "    \"\"\"MLP, das die Softmax-Kostenfunktion mit dem Gradientenverfahren optimiert.\n",
        "    X: Eingabedatem\n",
        "    y: Labels\n",
        "    layers_dims: Liste mit en Dimensionen der Schichten\n",
        "    alpha(default = 0.01): Lernrate\n",
        "    epoch(default = 20): Anzahl der zu trainierenden Epochen\n",
        "    batch_size(default = 400): Größe eines Minibatches\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "    parameters = init(layers_dims)\n",
        "    b, W = parameters\n",
        "\n",
        "    for i in range(epoch):\n",
        "\n",
        "        for batch in range(0,len(y),batch_size):\n",
        "            y_hat, caches = forward(X[batch:batch+batch_size], b, W)\n",
        "            cost = J(y_hat, y[batch:batch+batch_size])\n",
        "            gradss = grads(y_hat, y[batch:batch+batch_size], caches)\n",
        "            parameters = optim(parameters, gradss, alpha)\n",
        "            b, W = parameters\n",
        "\n",
        "        costs.append(cost.item(0))\n",
        "        print(f'Epoch {i}: {cost.item(0)}')\n",
        "\n",
        "    plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
        "    plt.show()\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sEAZsa3cqCb"
      },
      "outputs": [],
      "source": [
        "trained_weights_10 = model(X_train, y_train, [28*28, 10], 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz1c9m5wcqCc"
      },
      "outputs": [],
      "source": [
        "class mlp_model:\n",
        "\n",
        "    def __init__(self, X, y, layers_dims):\n",
        "        \"\"\"MLP Modell\n",
        "        X: Eingabedatem\n",
        "        y: Labels\n",
        "        layers_dims: Liste mit en Dimensionen der Schichten\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.costs = []\n",
        "        self.parameters = init(layers_dims)\n",
        "\n",
        "    def fit(self, alpha = 0.01, epoch = 20, batch_size = 400):\n",
        "        \"\"\"MLP, das die Softmax-Kostenfunktion mit dem Gradientenverfahren optimiert.\n",
        "        alpha(default = 0.01): Lernrate\n",
        "        epoch(default = 20): Anzahl der zu trainierenden Epochen\n",
        "        batch_size(default = 400): Größe eines Minibatches\n",
        "        \"\"\"\n",
        "\n",
        "        b, W = self.parameters\n",
        "\n",
        "        for i in range(epoch):\n",
        "\n",
        "            for batch in range(0,len(self.y),batch_size):\n",
        "                y_hat, caches = forward(self.X[batch:batch+batch_size], b, W)\n",
        "                cost = J(y_hat, self.y[batch:batch+batch_size])\n",
        "                gradss = grads(y_hat, self.y[batch:batch+batch_size], caches)\n",
        "                self.parameters = optim(self.parameters, gradss, alpha)\n",
        "                b, W = self.parameters\n",
        "\n",
        "            self.costs.append(cost.item(0))\n",
        "            print(f'Epoch {i}: {cost.item(0):.5}')\n",
        "\n",
        "        plt.plot(range(1,len(self.costs)),self.costs[1:], \"x-\")\n",
        "        plt.show()\n",
        "        return self.parameters\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Vorhersagefunktion für das trainierte Modell.\"\"\"\n",
        "\n",
        "        b, W = self.parameters\n",
        "        y_pred, _ = forward(X, b, W)\n",
        "\n",
        "        return (y_pred>=0.5)*1\n",
        "\n",
        "    def accuracy(y_test, y_pred_0):\n",
        "        return 100-np.sum(np.abs(y_pred_0-y_test))*100/len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsLJDH58cqCc"
      },
      "outputs": [],
      "source": [
        "model_10 = mlp_model(X_train, y_train, [28*28, 10])\n",
        "trained_weights_10 = model_10.fit(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzLJeVwhcqCc"
      },
      "outputs": [],
      "source": [
        "y_pred_10 = model_10.predict(X_test)\n",
        "mlp_model.accuracy(y_test, y_pred_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWXwZ2BpcqCd"
      },
      "outputs": [],
      "source": [
        "model_20_10 = mlp_model(X_train, y_train, [28*28, 20, 10])\n",
        "trained_weights_20_10 = model_20_10.fit(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0huKRICcqCd"
      },
      "outputs": [],
      "source": [
        "y_pred_20_10 = model_20_10.predict(X_test)\n",
        "mlp_model.accuracy(y_test, y_pred_20_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RlB4mixcqCd"
      },
      "source": [
        "# Neuronale Netze mit Keras\n",
        "\n",
        "In dieser Aufgabenblatt geht es darum, ein einfachen Künstliches Neuronales Netz für ein Regressionsproblem mit der Keras API zu implementieren.\n",
        "\n",
        "Wir verwenden den *Boston Housing* Datensatz, der als Standard-Anwendungsbeispiel über die Keras API heruntergeladen werden kann.\n",
        "\n",
        "Der Datensatz beschreibt die Wohnverhältnisse in verschieden Gebieten um Boston in den 1970er Jahren.\n",
        "Er enthält 506 Einträge mit jeweils 13 numerischen Merkmalen, die Zielvariable beschreibt den mittleren Wert der Häuserpreise in dem jeweiligen Bezirk (in Tausend USD).\n",
        "\n",
        "Gute Modelle sollten Vorhersagen mit einer mittlere quadratische Abweichung (*mean squared error*, MSE) unterhalb 20 (Tausend USD) treffen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfSyslEycqCd"
      },
      "source": [
        "Zuerst binden wir die wichtigsten Bibliotheken ein und laden den Datensatz mit der Funktion `boston_housing.load_data()` herunter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQIs-SYqcqCd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwIZWl16cqCd"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = datasets.boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw1Z-CfucqCd"
      },
      "outputs": [],
      "source": [
        "mean = X_train.mean(axis=0)\n",
        "X_train -= mean\n",
        "std = X_train.std(axis=0)\n",
        "X_train /= std\n",
        "\n",
        "X_test -= mean\n",
        "X_test /= std\n",
        "\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CZSnO1VcqCe"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPqx1Pv6cqCe"
      },
      "source": [
        "Nachdem die Modellparameter zusammengestellt sind, muss die Eigentliche Modellfunktion erstellt werden.\n",
        "In Tensorflow wird damit der Graph des Modells erzeugt.\n",
        "Die `compile`-Funktion kann mit verschiedenen Parametern angepasst werden:\n",
        "- `optimizer` legt die Art (den Algorithmus) des Gradientenverfahrens fest. (https://keras.io/optimizers/)\n",
        "- `loss` bestimmt die Art der Kostenfunktion (https://keras.io/losses/)\n",
        "- `metrics` legt fest, welche Qualitätsmerkmale beim Trainieren überprüft werden sollen (https://keras.io/metrics/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qb27yxtcqCe"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='SGD',\n",
        "    loss='mse',\n",
        "    metrics=['mae','mape'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiKZbbJbcqCe"
      },
      "source": [
        "Um die Ergebnisse zu visulaisieren, legen wir ein Log-Verzeichnis pro Lauf des Verfahrens an, in das wir die Informationen der `fit`-Funktion schreiben.\n",
        "Damit können wir später die Ergebnisse der einzelnen Trainingsläufe vergeleichen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK-OHNA7cqCe"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "newlog = '.\\\\keras_data\\\\run_' + time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=newlog, histogram_freq=0, write_graph=True, write_images=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEiMVlBcqCe"
      },
      "source": [
        "Nun rufen wir die `fit`-Funktion auf, um die Modellparameter zu lernen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS7AvmiucqCe"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    callbacks=[tbCallBack]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ7b2iuBcqCe"
      },
      "source": [
        "Nach dem Training verwenden wir die Testdaten, um unser trainiertes Modell zu evaluieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ9236ADcqCe"
      },
      "outputs": [],
      "source": [
        "mse_score, mae_score, mape_score = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Mittlwert der Fehlerquadrate: \", mse_score)\n",
        "print(\"Mittlerer absoluter Fehler: \", mae_score)\n",
        "print(\"Mittlerer absoluter Fehler (in Prozent): \", mape_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEkWdeVcqCf"
      },
      "source": [
        "Das Trainierte Modell schätzt die Häuserpreise vermutlich nicht sonderlich gut.\n",
        "Dies liegt an der sehr unglücklichen Wahl der (Hyper-)Parameter.\n",
        "Versuchen Sie die Parameter sinnvoll anzupassen, sodass sich die Qualität der Schaätzfunktion verbessert.\n",
        "\n",
        "Um die Log-Dateien sowie den Modellgraphnen zu visualisieren, können die Tensorboard verwenden.\n",
        "Rufen Sie daszu (im Verzeichnis der *.ipynb* Datei) das Kommando `tensorboard --logdir=./keras_data` auf.\n",
        "Wenn Tensorboard gestartet ist, erreichen Sie die Web-Oberfläche unter der URL `localhost:8008`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4XioFzIcqCf"
      },
      "outputs": [],
      "source": [
        "for a in model.get_weights():\n",
        "    print(np.shape(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlzITAMucqCf"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrYZceA5cqCf"
      },
      "source": [
        "Wir verwenden nun in einem zweiten Beispiel Keras mit dem MNIST Datensatz:\n",
        "\n",
        "### Beschaffen der Daten und Vorverarbeiten\n",
        "\n",
        "1. Daten mit `datasets.mnist.load_data()` laden\n",
        "2. Bilder in eindimensionale Arrays umwandeln mit `reshape`. Teilen durch 255 um zu Normalisieren.\n",
        "3. One-Hot Encoding der Labels mit `utils.to_categorical`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a1780d72ee450607",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "GMEcUsK5cqCf"
      },
      "outputs": [],
      "source": [
        "# Data loading\n",
        "(X_train, y_train), (X_test, y_test) = (None, None), (None, None)\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b51934e76e8f110e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4uC9wLadcqCf"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "assert X_train.ptp() == X_test.ptp() == 1, 'Data is not nomalized'\n",
        "assert X_train.shape == (60000, 784)\n",
        "assert X_test.shape == (10000, 784)\n",
        "assert y_train.shape == (60000, 10), 'Make sure to one hot encode train labels'\n",
        "assert y_test.shape == (10000, 10), 'Make sure to one hot encode test labels'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAeAfIsjcqCf"
      },
      "source": [
        "### Das Modell aufstellen\n",
        "Erstellen Sie ein Modell mit geeigneten Hyperparametern:\n",
        "\n",
        "*Hinweise:*\n",
        "- Verwenden Sie die `softmax` Aktivierungsfunktion um die Ausgabeschicht in eine Wahrscheinlichkeitsfunktion umzuwandeln.\n",
        "- `input_shape` ist ein Vektor der Dimension `(784,)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ee3d61912367ba52",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "LxFcYOfNcqCf"
      },
      "outputs": [],
      "source": [
        "mnist_model = None\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-9f1b71a062361e10",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LKSyd28TcqCf"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "assert type(mnist_model) == models.Sequential\n",
        "assert mnist_model.built, 'The model is not built'\n",
        "assert len(mnist_model.layers) > 2, 'You should have at least one hidden layer'\n",
        "assert mnist_model.input_shape[1:] == X_train[0].shape, 'Input must match the features of the image'\n",
        "assert mnist_model.output_shape[1] == 10, 'Output must match the number of classes'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQQ1Y5IRcqCf"
      },
      "source": [
        "### Modell Kompilieren\n",
        "Verwenden Sie `compile` um den `optimizer`, die `loss` Funktion und die `metrics` festzulegen.\n",
        "\n",
        "*Hinweise:*\n",
        "- Da wir `softmax` verwenden, wählen wir die Kostenfunktion `categorical_crossentropy`.\n",
        "- `accuracy` ist an dieser Stelle eine gute Metrik für die Klassifikation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f41104f1f759e66f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "G5ABd8mFcqCg"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-6e6195d6ba6fa627",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "v09fX8AfcqCg"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "\n",
        "assert mnist_model.loss\n",
        "assert mnist_model.optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keBKOUBgcqCg"
      },
      "source": [
        "### Training\n",
        "Trainieren Sie das Modell mit der `fit` Funktion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9c7096937405635f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "jhahkmTScqCg"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-fc3dc3a7dd085783",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "gO3Uuy4hcqCg"
      },
      "outputs": [],
      "source": [
        "# Test Cell\n",
        "#----------\n",
        "\n",
        "assert len(mnist_model.history.epoch) >= 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF54C4C-cqCg"
      },
      "source": [
        "### Testen\n",
        "Evaluieren Sie das Modell mit der `evaluate` Funktion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-daf429a21c6e809b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "s67ln2qkcqCg"
      },
      "outputs": [],
      "source": [
        "loss, acc = [None]*2\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "print(\"loss: %.4f - accuracy: %.4f \" %(loss, acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWvgq3WscqCg"
      },
      "source": [
        "### Visualisierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2geAabwcqCg"
      },
      "outputs": [],
      "source": [
        "img_i = np.random.randint(10000, size=8)\n",
        "fig, ax = plt.subplots(1, 8, figsize=(18,3))\n",
        "for i, j in enumerate(img_i):\n",
        "    ax[i].set_title('True: %d\\nPredicted: %d' %(np.argmax(y_test[j]),\n",
        "                np.argmax(mnist_model.predict(X_test[j].reshape(1, -1)), axis=-1)))\n",
        "    ax[i].axis('off')\n",
        "    ax[i].imshow(X_test[j].reshape(28,28), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWM8_S8EcqCg"
      },
      "outputs": [],
      "source": [
        "mnist_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDItls35cqCh"
      },
      "source": [
        "## Referenzen:\n",
        "[1] M. Berthold, C. Borgelt, F. Höppner and F. Klawonn, Guide to Intelligent Data Analysis, London: Springer-Verlag, 2010.  \n",
        "[2] A. Ng, Machine Learning Yearning, deeplearning.ai, 2018.  \n",
        "\n",
        "**Tipp:** Schauen Sie sich die Playlist von *3Blue1Brown* über Neuronale Netze auf [Youtube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) an"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}